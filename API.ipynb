{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d3401d3-38d0-417b-b06a-e57f2c51e106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://100.91.93.250:5000\n",
      "Press CTRL+C to quit\n",
      "100.91.93.250 - - [23/Jun/2025 18:22:07] \"GET / HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_path = \"D:/DATA FOR RESEARCH PROJECT/biobert_pacemaker_final-20250623T084209Z-1-001/biobert_pacemaker_final\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, use_safetensors=True)\n",
    "model.eval()\n",
    "\n",
    "# Flask app\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    data = request.get_json()\n",
    "    text = data.get(\"text\", \"\")\n",
    "    if not text:\n",
    "        return jsonify({\"error\": \"No text provided\"}), 400\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "        pred = torch.argmax(probs).item()\n",
    "        conf = probs[0][pred].item()\n",
    "\n",
    "    return jsonify({\n",
    "        \"prediction\": \"Adverse Event\" if pred == 1 else \"Not Adverse\",\n",
    "        \"confidence\": round(conf, 3)\n",
    "    })\n",
    "\n",
    "from flask import Flask, request\n",
    "\n",
    "@app.route(\"/\", methods=[\"GET\"])\n",
    "def home():\n",
    "    return \"\"\"\n",
    "    <h2>BioBERT Adverse Event Classifier</h2>\n",
    "    <p>Use the <code>/predict</code> endpoint via POST to get predictions.</p>\n",
    "    <p>Example: send JSON <code>{\"text\": \"Patient experienced pacing failure\"}</code></p>\n",
    "    \"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host=\"0.0.0.0\", port=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca0e16d8-9a17-40f7-82bc-4afea0c57086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://100.91.93.250:5000\n",
      "Press CTRL+C to quit\n",
      "100.91.93.250 - - [23/Jun/2025 19:09:20] \"GET / HTTP/1.1\" 200 -\n",
      "100.91.93.250 - - [23/Jun/2025 19:10:30] \"POST / HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_path = \"D:/DATA FOR RESEARCH PROJECT/biobert_pacemaker_final-20250623T084209Z-1-001/biobert_pacemaker_final\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, use_safetensors=True)\n",
    "model.eval()\n",
    "\n",
    "# Flask app\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    data = request.get_json()\n",
    "    text = data.get(\"text\", \"\")\n",
    "    if not text:\n",
    "        return jsonify({\"error\": \"No text provided\"}), 400\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "        pred = torch.argmax(probs).item()\n",
    "        conf = probs[0][pred].item()\n",
    "\n",
    "    return jsonify({\n",
    "        \"prediction\": \"Adverse Event\" if pred == 1 else \"Not Adverse\",\n",
    "        \"confidence\": round(conf, 3)\n",
    "    })\n",
    "\n",
    "from flask import Flask, request\n",
    "\n",
    "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
    "def home():\n",
    "    if request.method == \"POST\":\n",
    "        text = request.form.get(\"text\")\n",
    "        if text:\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "                pred = torch.argmax(probs).item()\n",
    "                conf = probs[0][pred].item()\n",
    "            return f\"<h3>Prediction:</h3> {'Adverse Event' if pred == 1 else 'Not Adverse'}<br><b>Confidence:</b> {round(conf, 3)}\"\n",
    "    return '''\n",
    "        <form method=\"post\">\n",
    "            <textarea name=\"text\" rows=\"6\" cols=\"60\" placeholder=\"Paste FOI_TEXT here...\"></textarea><br>\n",
    "            <input type=\"submit\">\n",
    "        </form>\n",
    "    '''\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host=\"0.0.0.0\", port=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98c5241d-26e9-4f38-8936-5fbf8ccad57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# url = \"http://127.0.0.1:5000/predict\"  # or use http://100.91.93.250:5000 if needed\n",
    "# data = {\"text\": \"The pacemaker battery failed during surgery.\"}\n",
    "\n",
    "# response = requests.post(url, json=data)\n",
    "# print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d27aca9-2c72-4c91-ab72-65c6f8df0641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Flask API after temperture scaling was done on biobert\n",
    "# from flask import Flask, request, jsonify\n",
    "# from flask_cors import CORS\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# app = Flask(__name__)\n",
    "# CORS(app)\n",
    "\n",
    "# # ðŸ”§ CONFIGURATION\n",
    "# model_path = r\"D:/DATA FOR RESEARCH PROJECT/flask API/deployed_model\"\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # 1. Load the config â€“ this defines the model_type (\"bert\")\n",
    "# config = AutoConfig.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "# # 2. Load the tokenizer using that config\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True, config=config)\n",
    "\n",
    "# # 3. Load the classification model\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     model_path,\n",
    "#     config=config,\n",
    "#     local_files_only=True,\n",
    "#     ignore_mismatched_sizes=True\n",
    "# )\n",
    "# model.to(device).eval()\n",
    "\n",
    "# # Temperature scaler (must match saved version)\n",
    "# class TemperatureScaler(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.temperature = torch.nn.Parameter(torch.tensor(1.0))  # 0-dim\n",
    "\n",
    "#     def forward(self, logits):\n",
    "#         return logits / self.temperature\n",
    "\n",
    "# temp_scaler = TemperatureScaler().to(device)\n",
    "# temp_scaler.load_state_dict(torch.load(\"temp_scaler.pth\", map_location=device))\n",
    "# temp_scaler.eval()\n",
    "\n",
    "# # ðŸš€ PREDICTION ENDPOINT (JSON POST)\n",
    "# @app.route('/predict', methods=['POST'])\n",
    "# def predict():\n",
    "#     data = request.get_json() or {}\n",
    "#     text = data.get(\"text\", \"\").strip()\n",
    "#     if not text:\n",
    "#         return jsonify({\"error\": \"No text provided\"}), 400\n",
    "\n",
    "#     inputs = tokenizer(\n",
    "#         text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512\n",
    "#     ).to(device)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         logits = model(**inputs).logits\n",
    "#         scaled_logits = temp_scaler(logits)\n",
    "#         probs = F.softmax(scaled_logits, dim=1)[0]\n",
    "#         pred = torch.argmax(probs).item()\n",
    "#         conf = probs[pred].item()\n",
    "\n",
    "#     return jsonify({\n",
    "#         \"prediction\": \"Adverse Event\" if pred == 1 else \"Not Adverse\",\n",
    "#         \"confidence\": round(conf, 3)\n",
    "#     })\n",
    "\n",
    "# # ðŸ–¥ FORM-BASED UI (browser)\n",
    "# @app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
    "# def home():\n",
    "#     if request.method == \"POST\":\n",
    "#         text = request.form.get(\"text\", \"\").strip()\n",
    "#         if text:\n",
    "#             inputs = tokenizer(\n",
    "#                 text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512\n",
    "#             ).to(device)\n",
    "#             with torch.no_grad():\n",
    "#                 logits = model(**inputs).logits\n",
    "#                 scaled_logits = temp_scaler(logits)\n",
    "#                 probs = F.softmax(scaled_logits, dim=1)[0]\n",
    "#                 pred = torch.argmax(probs).item()\n",
    "#                 conf = probs[pred].item()\n",
    "\n",
    "#             return f\"\"\"\n",
    "#                 <h3>Prediction: {'Adverse Event' if pred == 1 else 'Not Adverse'}</h3>\n",
    "#                 <b>Confidence:</b> {round(conf, 3)}\n",
    "#                 <br><a href=\"/\">Try again</a>\n",
    "#             \"\"\"\n",
    "#     return '''\n",
    "#         <form method=\"post\">\n",
    "#             <textarea name=\"text\" rows=\"6\" cols=\"60\"\n",
    "#                 placeholder=\"Paste FOI_TEXT here...\"></textarea><br>\n",
    "#             <input type=\"submit\" value=\"Classify\">\n",
    "#         </form>\n",
    "#     '''\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     app.run(host=\"0.0.0.0\", port=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80119995-09e5-49df-a17b-a29194a52718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.1.150:5000\n",
      "Press CTRL+C to quit\n",
      "192.168.1.150 - - [13/Jul/2025 21:41:13] \"GET / HTTP/1.1\" 200 -\n",
      "192.168.1.150 - - [13/Jul/2025 21:41:13] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
      "192.168.1.150 - - [13/Jul/2025 21:44:00] \"POST / HTTP/1.1\" 200 -\n",
      "192.168.1.150 - - [13/Jul/2025 21:44:03] \"GET / HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app, origins=['http://localhost:3000'], methods=['POST'])\n",
    "#CORS(app)\n",
    "\n",
    "# CONFIGURATION\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = r\"D:/DATA FOR RESEARCH PROJECT/biobert_pacemaker_final-20250623T084209Z-1-001/biobert_pacemaker_final\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "# Load model â€“ ensure classifier head matches:\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_path,\n",
    "    local_files_only=True,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "model.to(device).eval()\n",
    "\n",
    "# Temperature scaler (must match saved version)\n",
    "class TemperatureScaler(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.temperature = torch.nn.Parameter(torch.tensor(1.0))  # 0-dim\n",
    "\n",
    "    def forward(self, logits):\n",
    "        return logits / self.temperature\n",
    "\n",
    "temp_scaler = TemperatureScaler().to(device)\n",
    "temp_scaler.load_state_dict(torch.load(\"D:/DATA FOR RESEARCH PROJECT/biobert_pacemaker_final-20250623T084209Z-1-001/biobert_pacemaker_final/temp_scaler.pth\", map_location=device))\n",
    "temp_scaler.eval()\n",
    "\n",
    "# PREDICTION ENDPOINT (JSON POST)\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    data = request.get_json() or {}\n",
    "    text = data.get(\"text\", \"\").strip()\n",
    "    if not text:\n",
    "        return jsonify({\"error\": \"No text provided\"}), 400\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        scaled_logits = temp_scaler(logits)\n",
    "        probs = F.softmax(scaled_logits, dim=1)[0]\n",
    "        pred = torch.argmax(probs).item()\n",
    "        conf = probs[pred].item()\n",
    "\n",
    "    return jsonify({\n",
    "        \"prediction\": \"Adverse Event\" if pred == 1 else \"Not Adverse\",\n",
    "        \"confidence\": round(conf, 3)\n",
    "    })\n",
    "\n",
    "# FORM-BASED UI (browser)\n",
    "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
    "def home():\n",
    "    if request.method == \"POST\":\n",
    "        text = request.form.get(\"text\", \"\").strip()\n",
    "        if text:\n",
    "            inputs = tokenizer(\n",
    "                text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512\n",
    "            ).to(device)\n",
    "            with torch.no_grad():\n",
    "                logits = model(**inputs).logits\n",
    "                scaled_logits = temp_scaler(logits)\n",
    "                probs = F.softmax(scaled_logits, dim=1)[0]\n",
    "                pred = torch.argmax(probs).item()\n",
    "                conf = probs[pred].item()\n",
    "\n",
    "            return f\"\"\"\n",
    "                <h3>Prediction: {'Adverse Event' if pred == 1 else 'Not Adverse'}</h3>\n",
    "                <b>Confidence:</b> {round(conf, 3)}\n",
    "                <br><a href=\"/\">Try again</a>\n",
    "            \"\"\"\n",
    "    return '''\n",
    "        <form method=\"post\">\n",
    "            <textarea name=\"text\" rows=\"6\" cols=\"60\"\n",
    "                placeholder=\"Paste FOI_TEXT here...\"></textarea><br>\n",
    "            <input type=\"submit\" value=\"Classify\">\n",
    "        </form>\n",
    "    '''\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health():\n",
    "    return jsonify(status=\"ok\", model_version=\"v1.0\"), 200\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host=\"0.0.0.0\", port=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b1f5350f-5856-4aef-8d65-3c6c2fac5718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: C:\\Users\\samridhi\\Research Project part A\n",
      "Files here: ['.ipynb_checkpoints', '0.26.0', \"0.26.0'\", 'API.ipynb', 'app_csv.ipynb', 'BertPacemaker.ipynb', 'EDA and Text analysis.ipynb', 'false_negatives.csv', 'false_positives.csv', 'flagged_adverse_cases.csv', 'my_plot.png', 'PacemakerAnalysis.ipynb', 'pacemakerClean-checkpoint.ipynb', 'Random Forest Classifier.ipynb', 'ResProject1-checkpoint.ipynb', 'results', 'Statisical Analysis.ipynb', 'Trend analysis.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"CWD:\", os.getcwd())\n",
    "print(\"Files here:\", os.listdir())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "338b2151-19a0-4ffa-8770-5dec07d7907d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists? True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict_path = r\"D:/DATA FOR RESEARCH PROJECT/biobert_pacemaker_final-20250623T084209Z-1-001/biobert_pacemaker_final/temp_scaler.pth\"\n",
    "print(\"Exists?\", os.path.exists(state_dict_path))\n",
    "temp_scaler.load_state_dict(torch.load(state_dict_path, map_location=device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1c7f8c-dbc5-45c1-a1d3-a4df87d17aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c487d8-9101-4673-be5d-17e42e828eb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9933c4e6-3d16-4f60-9b69-796401c6ae17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
